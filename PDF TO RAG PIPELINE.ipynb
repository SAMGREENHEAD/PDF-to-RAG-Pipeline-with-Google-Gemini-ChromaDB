{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11323729,"sourceType":"datasetVersion","datasetId":7082723}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:19.595904Z","iopub.execute_input":"2025-09-05T12:56:19.596503Z","iopub.status.idle":"2025-09-05T12:56:19.864224Z","shell.execute_reply.started":"2025-09-05T12:56:19.596476Z","shell.execute_reply":"2025-09-05T12:56:19.863638Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/constitution/swaziland_constitution.pdf\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Optional: free some space in Kaggle image\n!pip uninstall -qqy jupyterlab kfp -y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:21.613625Z","iopub.execute_input":"2025-09-05T12:56:21.614505Z","iopub.status.idle":"2025-09-05T12:56:23.847814Z","shell.execute_reply.started":"2025-09-05T12:56:21.614480Z","shell.execute_reply":"2025-09-05T12:56:23.847029Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ================================\n# 📦 Install Required Libraries\n# ================================\n# This cell installs the dependencies needed for:\n# 1. google-genai (v1.7.0)  →  Accessing Google’s Generative AI models (Gemini, etc.)\n# 2. chromadb (v0.6.3)      →  A lightweight vector database for storing and retrieving embeddings\n# 3. pdfplumber             →  Extracting text from PDF files (preserves layout better than some alternatives)\n# 4. PyPDF2                 →  Another PDF processing library, useful for splitting/merging/reading PDFs\n# 5. ftfy                   →  \"Fixes Text For You\" — cleans messy text encoding issues (important for NLP tasks)\n\n# The \"-qU\" flags mean:\n# -q  → Quiet mode (less output in the notebook)\n# -U  → Upgrade to the latest version if already installed\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\" pdfplumber PyPDF2 ftfy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:23.849457Z","iopub.execute_input":"2025-09-05T12:56:23.849723Z","iopub.status.idle":"2025-09-05T12:56:27.782112Z","shell.execute_reply.started":"2025-09-05T12:56:23.849702Z","shell.execute_reply":"2025-09-05T12:56:27.781055Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ==========================================\n# 📚 Import Required Python Libraries\n# ==========================================\n\n# --- Built-in Python modules ---\nimport os          # Work with operating system features (paths, environment variables)\nimport re          # Regular expressions for text pattern matching/cleaning\nimport json        # Work with JSON data (reading/writing configurations, API responses)\nimport unicodedata # Handle and normalize Unicode characters (useful for cleaning text)\nimport uuid        # Generate unique identifiers (e.g., for document IDs)\nimport textwrap    # Format and wrap text neatly (useful when printing long text blocks)\n\n# dataclasses: Helps define classes for structured data with less boilerplate code\nfrom dataclasses import dataclass, field\n\n# typing: Used for type hints (makes code more readable and easier to debug)\nfrom typing import List, Dict, Any, Optional\n\n\n# --- Google Generative AI (Gemini) SDK ---\nfrom google import genai                   # Main SDK for interacting with Google's Generative AI models\nfrom google.genai import types             # Provides structured request/response types for API calls\nfrom google.api_core import retry          # Enables automatic retries for failed API calls (network safe)\n\n\n# --- PDF Processing Libraries ---\nimport pdfplumber                          # Extracts text from PDFs while preserving layout/structure\nimport PyPDF2                              # Additional PDF operations: merging, splitting, metadata access\nfrom ftfy import fix_text                  # \"Fixes Text For You\" — cleans broken or misencoded text\n\n\n# --- ChromaDB (Vector Database for RAG) ---\nimport chromadb                            # Main library for working with vector storage and retrieval\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\n# (Documents, EmbeddingFunction, Embeddings help us work with text collections and custom embeddings)\n\n\n# --- Kaggle Secrets ---\nfrom kaggle_secrets import UserSecretsClient\n# Allows secure storage and retrieval of API keys/secrets when running code on Kaggle notebooks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.783268Z","iopub.execute_input":"2025-09-05T12:56:27.783591Z","iopub.status.idle":"2025-09-05T12:56:27.790135Z","shell.execute_reply.started":"2025-09-05T12:56:27.783568Z","shell.execute_reply":"2025-09-05T12:56:27.789367Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# --- Paths ---\nPDF_PATH = \"/kaggle/input/constitution/swaziland_constitution.pdf\"  \nOUT_DIR = \"/kaggle/working\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.791726Z","iopub.execute_input":"2025-09-05T12:56:27.791954Z","iopub.status.idle":"2025-09-05T12:56:27.811612Z","shell.execute_reply.started":"2025-09-05T12:56:27.791929Z","shell.execute_reply":"2025-09-05T12:56:27.810849Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\nMD_OUT = os.path.join(OUT_DIR, \"constitution_clean.md\")\nSTRUCT_JSON_OUT = os.path.join(OUT_DIR, \"constitution_structured.json\")\nCHUNKS_JSONL_OUT = os.path.join(OUT_DIR, \"constitution_chunks.jsonl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.812389Z","iopub.execute_input":"2025-09-05T12:56:27.812603Z","iopub.status.idle":"2025-09-05T12:56:27.827651Z","shell.execute_reply.started":"2025-09-05T12:56:27.812586Z","shell.execute_reply":"2025-09-05T12:56:27.826933Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nDOC_ID = \"swaziland_constitution_2005\"\nSOURCE_FILE = os.path.basename(PDF_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.828456Z","iopub.execute_input":"2025-09-05T12:56:27.828711Z","iopub.status.idle":"2025-09-05T12:56:27.843047Z","shell.execute_reply.started":"2025-09-05T12:56:27.828688Z","shell.execute_reply":"2025-09-05T12:56:27.842340Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Chunk sizes (approx words-as-tokens)\nTARGET_TOKENS = 600\nOVERLAP_TOKENS = 90","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.843824Z","iopub.execute_input":"2025-09-05T12:56:27.844078Z","iopub.status.idle":"2025-09-05T12:56:27.858251Z","shell.execute_reply.started":"2025-09-05T12:56:27.844055Z","shell.execute_reply":"2025-09-05T12:56:27.857470Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Chroma collection name\nDB_NAME = \"swazi_constitution_chroma\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.859023Z","iopub.execute_input":"2025-09-05T12:56:27.859211Z","iopub.status.idle":"2025-09-05T12:56:27.872103Z","shell.execute_reply.started":"2025-09-05T12:56:27.859196Z","shell.execute_reply":"2025-09-05T12:56:27.871475Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\n# --- Gemini API Key ---\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nassert GOOGLE_API_KEY, \"Please add a Kaggle Secret named GOOGLE_API_KEY and enable it for this notebook.\"\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:27.872920Z","iopub.execute_input":"2025-09-05T12:56:27.873147Z","iopub.status.idle":"2025-09-05T12:56:28.051118Z","shell.execute_reply.started":"2025-09-05T12:56:27.873132Z","shell.execute_reply":"2025-09-05T12:56:28.050528Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ==========================================\n# 🧹 Text Normalization & Cleaning Functions\n# ==========================================\n\ndef normalize_text(s: str) -> str:\n    \"\"\"\n    Cleans and normalizes raw text strings by:\n    1. Normalizing Unicode characters (NFKC form)\n    2. Fixing common text encoding issues (using ftfy)\n    3. Removing soft hyphens (\\u00AD) which break words across lines\n    4. Replacing non-breaking spaces (\\xa0) with normal spaces\n    5. Collapsing multiple spaces/tabs into a single space\n    6. Stripping leading/trailing whitespace\n    \"\"\"\n    s = unicodedata.normalize(\"NFKC\", s or \"\")\n    s = fix_text(s)\n    s = s.replace(\"\\u00AD\", \"\")  # remove soft hyphen\n    s = s.replace(\"\\xa0\", \" \")   # convert non-breaking space to normal space\n    s = re.sub(r\"[ \\t]+\", \" \", s).strip()\n    return s\n\n\ndef looks_like_header_footer(line: str) -> bool:\n    \"\"\"\n    Detects whether a given line is likely a header/footer (noise)\n    that should be ignored when extracting text from PDFs.\n\n    Rules:\n    - If line is empty → not a header/footer.\n    - If line is just a page number (1–3 digits) → header/footer.\n    - If line matches known \"document noise\" strings → header/footer.\n    - If line consists entirely of underscores → header/footer.\n    \"\"\"\n    if not line:\n        return False\n\n    # Detect page numbers (e.g., \"12\")\n    if re.fullmatch(r\"[0-9]{1,3}\", line):\n        return True\n\n    # Known header/footer noise strings\n    doc_noise = {\n        \"Swaziland - Constitution 2005\",\n        \"THE CONSTITUTION OF THE KINGDOM OF SWAZILAND ACT, 2005\",\n        \"Arrangement of sections\",\n        \"__________________\",\n        \"___________________\",\n    }\n    if line in doc_noise:\n        return True\n\n    # Detect lines made only of underscores\n    if re.fullmatch(r\"_+\", line):\n        return True\n\n    return False\n\n\ndef merge_hyphenated_lines(lines: List[str]) -> List[str]:\n    \"\"\"\n    Merges lines that were split across pages/lines using hyphenation.\n\n    Example:\n    \"inter-\" (end of line) + \"national\" (next line) → \"international\"\n    \"\"\"\n    merged = []\n    i = 0\n    while i < len(lines):\n        line = lines[i]\n        # Look ahead at next line (avoid going out of bounds)\n        if i < len(lines) - 1:\n            nxt = lines[i + 1]\n            # If current line ends with a hyphenated word and next line starts lowercase → merge\n            if re.search(r\"[A-Za-z]-$\", line) and re.match(r\"^[a-z].*\", nxt):\n                merged.append(line[:-1] + nxt)  # remove hyphen and join\n                i += 2\n                continue\n        merged.append(line)\n        i += 1\n    return merged\n\n\ndef rebuild_paragraphs(clean_lines: List[str]) -> List[str]:\n    \"\"\"\n    Rebuilds full paragraphs from cleaned lines of text.\n\n    Logic:\n    - Lines are grouped into paragraphs until a blank line or\n      a special \"section indicator\" (e.g., numbering or CHAPTER title) is found.\n    - Each paragraph is joined with spaces and extra whitespace is removed.\n\n    This is crucial to make the text readable and ready for embeddings.\n    \"\"\"\n\n    paragraphs, buf = [], []\n\n    # Helper function to flush the buffer into a paragraph\n    def flush():\n        if not buf:\n            return\n        para = \" \".join(buf)\n        para = re.sub(r\" +\", \" \", para).strip()\n        if para:\n            paragraphs.append(para)\n        buf.clear()\n\n    for line in clean_lines:\n        # If line is blank, treat as paragraph break\n        if not line.strip():\n            flush()\n            continue\n\n        # If line starts with numbering or \"CHAPTER\", treat as its own paragraph\n        if re.match(r\"^(\\(\\d+\\)|\\([a-z]\\)|\\([ivx]+\\)|\\d+\\.)\\s\", line, re.I) or re.match(r\"^CHAPTER(\\s+|$)\", line, re.I):\n            flush()\n            paragraphs.append(line.strip())\n        else:\n            buf.append(line.strip())\n\n    flush()  # flush remaining buffer\n    return paragraphs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T12:56:28.082527Z","iopub.execute_input":"2025-09-05T12:56:28.082815Z","iopub.status.idle":"2025-09-05T12:56:28.092789Z","shell.execute_reply.started":"2025-09-05T12:56:28.082778Z","shell.execute_reply":"2025-09-05T12:56:28.092080Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ==========================================\n# 📄 Extract and Clean Text from a PDF\n# ==========================================\n\ndef extract_pages(pdf_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Extracts text from each page of a PDF and returns a list of cleaned paragraphs.\n\n    Parameters:\n    ----------\n    pdf_path : str\n        The file path to the PDF document.\n\n    Returns:\n    -------\n    List[Dict[str, Any]]:\n        A list of dictionaries, where each dictionary contains:\n        {\n            \"page\": page_number,\n            \"text\": cleaned_paragraph_text\n        }\n\n    Key Steps:\n    ----------\n    1. Try using `pdfplumber` first for better layout-preserving text extraction.\n    2. If pdfplumber fails (e.g., due to file corruption), fallback to `PyPDF2`.\n    3. Normalize and clean text line-by-line:\n        - Remove headers, footers, page numbers\n        - Merge hyphenated words split across lines\n        - Remove decorative lines (underscores, dashes)\n    4. Rebuild lines into paragraphs for semantic completeness.\n    \"\"\"\n\n    pages_clean: List[Dict[str, Any]] = []  # Holds final output\n\n    try:\n        # --- Primary Extraction with pdfplumber ---\n        with pdfplumber.open(pdf_path) as pdf:\n            for pno, page in enumerate(pdf.pages, start=1):\n                # Extract text with a small tolerance to preserve word grouping\n                raw = page.extract_text(x_tolerance=2, y_tolerance=2) or \"\"\n                raw = normalize_text(raw)\n\n                # Split into lines and trim whitespace\n                raw_lines = [l.rstrip() for l in raw.splitlines()]\n                content_lines = []\n\n                for ln in raw_lines:\n                    line = ln.strip()\n\n                    # Skip headers/footers and page numbers\n                    if looks_like_header_footer(line):\n                        continue\n                    if re.fullmatch(r\"(Page\\s+)?\\d{1,3}\", line, re.I):\n                        continue\n\n                    content_lines.append(line)\n\n                # Merge words split by hyphens across lines\n                content_lines = merge_hyphenated_lines(content_lines)\n\n                # Remove decorative lines (e.g., \"___\" or \"---\")\n                content_lines = [re.sub(r\"^[_\\-]{3,}$\", \"\", ln) for ln in content_lines]\n\n                # Rebuild paragraphs\n                paragraphs = rebuild_paragraphs(content_lines)\n\n                # Store results with page number\n                for para in paragraphs:\n                    pages_clean.append({\"page\": pno, \"text\": para})\n\n    except Exception as e:\n        # --- Fallback to PyPDF2 if pdfplumber fails ---\n        print(f\"[warn] pdfplumber failed ({e}); falling back to PyPDF2...\")\n\n        reader = PyPDF2.PdfReader(pdf_path)\n        for pno, page in enumerate(reader.pages, start=1):\n            raw = page.extract_text() or \"\"\n            raw = normalize_text(raw)\n            raw_lines = [l.rstrip() for l in raw.splitlines()]\n            content_lines = []\n\n            for ln in raw_lines:\n                line = ln.strip()\n\n                # Skip headers/footers and page numbers\n                if looks_like_header_footer(line):\n                    continue\n                if re.fullmatch(r\"(Page\\s+)?\\d{1,3}\", line, re.I):\n                    continue\n\n                content_lines.append(line)\n\n            # Same cleaning steps as above\n            content_lines = merge_hyphenated_lines(content_lines)\n            content_lines = [re.sub(r\"^[_\\-]{3,}$\", \"\", ln) for ln in content_lines]\n            paragraphs = rebuild_paragraphs(content_lines)\n\n            for para in paragraphs:\n                pages_clean.append({\"page\": pno, \"text\": para})\n\n    return pages_clean\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:00:39.841941Z","iopub.execute_input":"2025-09-05T13:00:39.842273Z","iopub.status.idle":"2025-09-05T13:00:39.851993Z","shell.execute_reply.started":"2025-09-05T13:00:39.842248Z","shell.execute_reply":"2025-09-05T13:00:39.851240Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nimport re\n\n# ==========================================\n# 🧱 Data Models for the Document Hierarchy\n# ==========================================\n\n@dataclass\nclass Subsection:\n    \"\"\"A labeled clause within a Section, e.g., '(a) ...' or '(1) ...'.\"\"\"\n    label: str\n    text: str\n    page_refs: List[int] = field(default_factory=list)\n\n@dataclass\nclass Section:\n    \"\"\"A numbered section, e.g., '12. Right to ...', with optional text and subsections.\"\"\"\n    number: str\n    title: str\n    text: str = \"\"\n    subsections: List[Subsection] = field(default_factory=list)\n    page_refs: List[int] = field(default_factory=list)\n\n@dataclass\nclass Chapter:\n    \"\"\"A CHAPTER with a roman numeral label and a title, containing sections.\"\"\"\n    label: str\n    title: str\n    sections: List[Section] = field(default_factory=list)\n    page_refs: List[int] = field(default_factory=list)\n\n\n# ==========================================\n# 🧩 Parse Flat Paragraphs → Structured TOC\n# ==========================================\n\ndef parse_structure(pages_paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Parses a list of cleaned (page, paragraph) entries into a hierarchical structure:\n      Chapter → Section → Subsection, carrying page references at each level.\n    \"\"\"\n    chapters: List[Chapter] = []\n    current_chapter: Optional[Chapter] = None\n    current_section: Optional[Section] = None\n    pending_section_number: Optional[str] = None  # used when \"12.\" appears and title follows on next line\n\n    for entry in pages_paragraphs:\n        page = entry[\"page\"]\n        para = entry[\"text\"]\n\n        # -----------------------\n        # 1) Chapter detection\n        # -----------------------\n        m_ch = re.match(r\"^CHAPTER\\s+([IVXLCDM]+)(?:\\s*(.*))?$\", para, re.I)\n        if m_ch:\n            # New chapter resets section state\n            pending_section_number = None\n            current_section = None\n\n            roman = m_ch.group(1).upper()\n            rest = (m_ch.group(2) or \"\").strip(\" -\")\n\n            ch_title = rest if rest else \"\"\n            current_chapter = Chapter(\n                label=f\"CHAPTER {roman}\",\n                title=ch_title,\n                page_refs=[page],\n            )\n            chapters.append(current_chapter)\n            continue\n\n        # If the CHAPTER title is on the next line in ALL CAPS, capture it.\n        if current_chapter and not current_chapter.title:\n            if len(para.split()) > 2 and para.isupper():\n                current_chapter.title = para\n                current_chapter.page_refs.append(page)\n                continue\n\n        # -----------------------\n        # 2) Section detection\n        #    Format: \"12. Title...\"  OR  \"12.\" followed by next paragraph as title\n        # -----------------------\n        m_sec = re.match(r\"^(\\d{1,3})\\.\\s*(.*)$\", para)\n        if m_sec:\n            sec_no = m_sec.group(1)\n            tail = m_sec.group(2).strip()\n\n            if tail:\n                # Title provided on the same line\n                current_section = Section(number=sec_no, title=tail, text=\"\", page_refs=[page])\n\n                # Ensure we have a chapter container (fallback if none parsed yet)\n                if current_chapter is None:\n                    current_chapter = Chapter(label=\"CHAPTER ?\", title=\"\", sections=[], page_refs=[page])\n                    chapters.append(current_chapter)\n\n                current_chapter.sections.append(current_section)\n                pending_section_number = None\n                continue\n            else:\n                # Only the section number is present; capture and wait for next paragraph as title\n                pending_section_number = sec_no\n                current_section = None\n                continue\n\n        # If previous line had \"12.\" and THIS line is actually the section title\n        if pending_section_number and current_chapter:\n            current_section = Section(number=pending_section_number, title=para, text=\"\", page_refs=[page])\n            current_chapter.sections.append(current_section)\n            pending_section_number = None\n            continue\n\n        # -----------------------\n        # 3) Subsection detection\n        #    e.g., \"(1) ...\", \"(a) ...\", \"(iv) ...\"\n        # -----------------------\n        m_sub = re.match(r\"^\\(([0-9ivx]+|[a-z])\\)\\s*(.*)$\", para, re.I)\n        if m_sub and current_section:\n            label = m_sub.group(1)\n            content = m_sub.group(2).strip()\n            current_section.subsections.append(\n                Subsection(label=f\"({label})\", text=content, page_refs=[page])\n            )\n            continue\n\n        # -----------------------\n        # 4) Body text accumulation\n        # -----------------------\n        if current_section:\n            # Append paragraph text to the current section body\n            current_section.text = (current_section.text + \" \" + para).strip()\n            # Track page reference if new\n            if page not in current_section.page_refs:\n                current_section.page_refs.append(page)\n        elif current_chapter:\n            # If we have chapter context but no section, just keep page refs\n            if page not in current_chapter.page_refs:\n                current_chapter.page_refs.append(page)\n\n    # ----------------------------------\n    # Build serializable JSON-like dict\n    # ----------------------------------\n    # NOTE: The following two variables must be defined by your runtime:\n    #   DOC_ID      → unique identifier for this document\n    #   SOURCE_FILE → original path/filename for traceability\n    result = {\n        \"doc_id\": DOC_ID,\n        \"source_path\": SOURCE_FILE,\n        \"title\": \"The Constitution of the Kingdom of Swaziland, 2005\",\n        \"chapters\": [\n            {\n                \"label\": ch.label,\n                \"title\": ch.title,\n                \"page_refs\": ch.page_refs,\n                \"sections\": [\n                    {\n                        \"number\": sec.number,\n                        \"title\": sec.title,\n                        \"page_refs\": sec.page_refs,\n                        \"text\": sec.text,\n                        \"subsections\": [\n                            {\"label\": sub.label, \"text\": sub.text, \"page_refs\": sub.page_refs}\n                            for sub in sec.subsections\n                        ],\n                    }\n                    for sec in ch.sections\n                ],\n            }\n            for ch in chapters\n        ],\n    }\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:00:42.334975Z","iopub.execute_input":"2025-09-05T13:00:42.335250Z","iopub.status.idle":"2025-09-05T13:00:42.350906Z","shell.execute_reply.started":"2025-09-05T13:00:42.335231Z","shell.execute_reply":"2025-09-05T13:00:42.350351Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def split_into_chunks(struct: Dict[str, Any], target_tokens=600, overlap_tokens=90) -> List[Dict[str, Any]]:\n    \"\"\"\n    Build RAG-ready chunks from the structured doc:\n      - One chunk stream per Section (Chapter label + Section title as heading_path)\n      - Concatenate section body + subsection bullets\n      - If the text is longer than target_tokens, emit sliding windows with overlap\n\n    NOTE: Despite the param names, this function currently uses *word counts*,\n          not LLM tokens. See the optional tokenizer-based variant below.\n    \"\"\"\n    chunks: List[Dict[str, Any]] = []\n\n    # Simple \"tokenizer\": split on whitespace (words), fast and library-free\n    def words(s: str) -> List[str]:\n        return s.split()\n\n    for ch in struct[\"chapters\"]:\n        ch_label = ch[\"label\"]\n        ch_title = ch.get(\"title\", \"\")\n\n        for sec in ch[\"sections\"]:\n            # Human-friendly breadcrumb for inspection/UIs\n            heading_path = f\"{ch_label} - {sec['title']}\".strip(\" -\")\n\n            # Minimal but useful metadata for later filtering/citation\n            base_meta = {\n                \"doc_id\": struct[\"doc_id\"],\n                \"source_file\": struct[\"source_path\"],\n                \"chapter\": ch_label,\n                \"chapter_title\": ch_title,\n                \"section_number\": sec[\"number\"],\n                \"section_title\": sec[\"title\"],\n                \"page_refs\": sec.get(\"page_refs\", []),\n            }\n\n            # Build a single text stream: section body + each subsection as a prefixed line\n            parts: List[str] = []\n            if sec.get(\"text\"):\n                parts.append(sec[\"text\"].strip())\n\n            for sub in sec.get(\"subsections\", []):\n                label = sub.get(\"label\", \"\")\n                t = (sub.get(\"text\") or \"\").strip()\n                if t:\n                    parts.append(f\"{label} {t}\".strip())\n\n            full_text = \"\\n\".join(parts).strip()\n            if not full_text:\n                continue  # skip empty sections\n\n            w = words(full_text)\n\n            # Case 1: short enough → one chunk\n            if len(w) <= target_tokens:\n                chunks.append({\n                    \"chunk_id\": str(uuid.uuid4()),\n                    \"heading_path\": heading_path,\n                    \"text\": full_text,\n                    \"metadata\": base_meta\n                })\n            else:\n                # Case 2: long → sliding window with overlap\n                # step = non-overlapped advance size\n                step = max(1, target_tokens - overlap_tokens)\n\n                for start in range(0, len(w), step):\n                    window = w[start:start + target_tokens]\n                    if not window:\n                        break\n\n                    text_piece = \" \".join(window).strip()\n\n                    chunks.append({\n                        \"chunk_id\": str(uuid.uuid4()),\n                        \"heading_path\": heading_path,\n                        \"text\": text_piece,\n                        \"metadata\": base_meta\n                    })\n\n                    # If we've reached or passed the tail, stop\n                    if start + target_tokens >= len(w):\n                        break\n\n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:00:44.498769Z","iopub.execute_input":"2025-09-05T13:00:44.499558Z","iopub.status.idle":"2025-09-05T13:00:44.508211Z","shell.execute_reply.started":"2025-09-05T13:00:44.499532Z","shell.execute_reply":"2025-09-05T13:00:44.507484Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ==========================================\n# 🚀 Pipeline Orchestration (Extract → Parse → Chunk → Export)\n# ==========================================\n\n# 0) Sanity check: ensure the input PDF exists\nassert os.path.exists(PDF_PATH), f\"PDF not found at: {PDF_PATH}\"\n\n# 1) Extract\nprint(\"[pipeline] Extracting pages ...\")\npages_clean = extract_pages(PDF_PATH)\n# Count unique pages seen and total paragraphs emitted\nprint(f\"[pipeline] Pages extracted: {len({p['page'] for p in pages_clean})}, paragraphs: {len(pages_clean)}\")\n\n# 2) Parse structure (Chapter → Section → Subsection)\nprint(\"[pipeline] Parsing structure ...\")\nstructured = parse_structure(pages_clean)\nchapters_detected = len(structured[\"chapters\"])\ntotal_sections = sum(len(ch[\"sections\"]) for ch in structured[\"chapters\"])\nprint(f\"[pipeline] Chapters detected: {chapters_detected}, sections: {total_sections}\")\n\n# 3) Chunk for RAG / fine-tuning (token-aware splitting with overlap)\nprint(\"[pipeline] Splitting into chunks ...\")\n# NOTE: Requires you to define split_into_chunks(), TARGET_TOKENS, OVERLAP_TOKENS beforehand.\nchunks = split_into_chunks(\n    structured,\n    target_tokens=TARGET_TOKENS,\n    overlap_tokens=OVERLAP_TOKENS\n)\nprint(f\"[pipeline] Chunks emitted: {len(chunks)}\")\n\n# 4) Emit outputs (Markdown for human inspection, JSON for structure, JSONL for RAG)\nprint(\"[pipeline] Writing outputs ...\")\n\n# 4.a) Build a readable Markdown summary of the parsed structure\nmd_lines = [\n    f\"# {structured.get('title','Document')}\",\n    f\"_Source: {structured.get('source_path','')}_\",\n    \"\"\n]\n\nfor ch in structured[\"chapters\"]:\n    # Chapter header: \"## CHAPTER I: Preliminary\"\n    ch_header = f\"## {ch['label']}\"\n    if ch.get(\"title\"):\n        ch_header += f\": {ch['title'].title()}\"  # Title-case for visual consistency\n    md_lines.append(ch_header)\n    md_lines.append(\"\")\n\n    # Sections inside the chapter\n    for sec in ch[\"sections\"]:\n        # Section header: \"### 12. Protection of right to life\"\n        md_lines.append(f\"### {sec['number']}. {sec['title']}\")\n        md_lines.append(\"\")\n\n        # Optional body text of the section (wrapped to 100 chars)\n        if sec.get(\"text\"):\n            md_lines.append(textwrap.fill(sec[\"text\"], width=100))\n            md_lines.append(\"\")\n\n        # 🔧 BUGFIX: iterate section subsections (not chapter)\n        # Previously: for sub in ch.get(\"subsections\", []):\n        for sub in sec.get(\"subsections\", []):\n            line = f\"- **{sub['label']}** {sub['text']}\"\n            # Wrap with indentation for nice bullets\n            md_lines.append(textwrap.fill(line, subsequent_indent=\"  \", width=100))\n        md_lines.append(\"\")\n\n# 4.b) Ensure output directory exists\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# 4.c) Write Markdown\nwith open(MD_OUT, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(md_lines).strip() + \"\\n\")\n\n# 4.d) Write structured JSON (chapters/sections/subsections with page refs)\nwith open(STRUCT_JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n    json.dump(structured, f, ensure_ascii=False, indent=2)\n\n# 4.e) Write RAG chunks to JSONL (one JSON object per line)\nwith open(CHUNKS_JSONL_OUT, \"w\", encoding=\"utf-8\") as f:\n    for ch in chunks:\n        f.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n\nprint(\"[pipeline] Outputs written:\")\nprint(\"  - Markdown:\", MD_OUT)\nprint(\"  - Structured JSON:\", STRUCT_JSON_OUT)\nprint(\"  - RAG JSONL:\", CHUNKS_JSONL_OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:00:48.930852Z","iopub.execute_input":"2025-09-05T13:00:48.931569Z","iopub.status.idle":"2025-09-05T13:01:02.161350Z","shell.execute_reply.started":"2025-09-05T13:00:48.931544Z","shell.execute_reply":"2025-09-05T13:01:02.160572Z"}},"outputs":[{"name":"stdout","text":"[pipeline] Extracting pages ...\n[pipeline] Pages extracted: 159, paragraphs: 3393\n[pipeline] Parsing structure ...\n[pipeline] Chapters detected: 38, sections: 563\n[pipeline] Splitting into chunks ...\n[pipeline] Chunks emitted: 308\n[pipeline] Writing outputs ...\n[pipeline] Outputs written:\n  - Markdown: /kaggle/working/constitution_clean.md\n  - Structured JSON: /kaggle/working/constitution_structured.json\n  - RAG JSONL: /kaggle/working/constitution_chunks.jsonl\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import json\nfrom typing import List, Dict, Any\nfrom google.api_core import retry\nimport chromadb\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\n\n# Assumes these already exist in your runtime:\n# - genai, types, client       (google-genai client & types)\n# - DB_NAME: str               (name of the Chroma collection)\n# - chunks: List[Dict[str,Any]] (RAG-ready chunks with \"text\", \"chunk_id\", \"heading_path\", \"metadata\")\n\n# -------------------------------------------------------\n# 🔁 Retry predicate: retry only rate limit / service busy\n# -------------------------------------------------------\n# google-genai raises APIError with .code (HTTP status). We retry 429 & 503.\nis_retriable = lambda e: (\n    isinstance(e, genai.errors.APIError) and getattr(e, \"code\", None) in {429, 503}\n)\n\n# -------------------------------------------------------\n# 🔡 Gemini Embedding Function for Chroma\n# -------------------------------------------------------\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    \"\"\"\n    Minimal Chroma-compatible embedding function backed by Google Gemini embeddings.\n    - Supports batching (Gemini embed_content supports up to ~100 docs per call for this model).\n    - `document_mode=True` → use \"retrieval_document\"; `False` → \"retrieval_query\".\n    \"\"\"\n    document_mode = True\n    batch_size = 100  # Gemini embed_content limit per call (safe default)\n\n    @retry.Retry(predicate=is_retriable)  # automatic backoff on 429/503\n    def __call__(self, input: Documents) -> Embeddings:\n        # Chroma may pass bytes/None; ensure strings.\n        texts = [(\"\" if x is None else str(x)) for x in input]\n        task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n\n        all_vecs: List[List[float]] = []\n        for i in range(0, len(texts), self.batch_size):\n            batch = texts[i:i + self.batch_size]\n            # google-genai v1.7.0 style:\n            resp = client.models.embed_content(\n                model=\"models/text-embedding-004\",\n                contents=batch,\n                config=types.EmbedContentConfig(task_type=task),\n            )\n            # `resp.embeddings` is a list aligned to `batch`\n            all_vecs.extend([e.values for e in resp.embeddings])\n\n        return all_vecs\n\n\n# -------------------------------------------------------\n# 🧽 Metadata sanitizer: keep nested types as JSON strings\n# -------------------------------------------------------\ndef _sanitize_metadata(md: dict) -> dict:\n    \"\"\"\n    Chroma metadata entries must be JSON-serializable scalars.\n    We JSON-encode any lists/dicts so you can round-trip them later.\n    \"\"\"\n    out = {}\n    for k, v in md.items():\n        if isinstance(v, (list, dict)):\n            out[k] = json.dumps(v, ensure_ascii=False)  # preserve structure as string\n        elif isinstance(v, (str, int, float, bool)) or v is None:\n            out[k] = v\n        else:\n            out[k] = str(v)\n    return out\n\n\n# -------------------------------------------------------\n# 🗃️ Build (or open) the Chroma vector DB and ingest chunks\n# -------------------------------------------------------\ndef build_vector_db(chunks: List[Dict[str, Any]]):\n    embed_fn = GeminiEmbeddingFunction()\n    embed_fn.document_mode = True  # document vectors (switch to False for query vectors)\n\n    # NOTE:\n    # - chromadb.Client() is in-memory (ephemeral). Use PersistentClient()\n    #   if you want data to persist across runs, e.g.:\n    #   chroma_client = chromadb.PersistentClient(path=\"./chroma_store\")\n    chroma_client = chromadb.Client()\n\n    # Optional clean re-run (uncomment to drop collection on each run)\n    # try:\n    #     chroma_client.delete_collection(DB_NAME)\n    # except Exception:\n    #     pass\n\n    db = chroma_client.get_or_create_collection(\n        name=DB_NAME,\n        embedding_function=embed_fn,  # Chroma will call embed_fn(texts) automatically\n    )\n\n    # Prepare payloads\n    documents = [c[\"text\"] for c in chunks]\n    ids = [c[\"chunk_id\"] for c in chunks]  # must be unique\n    metadatas = [\n        _sanitize_metadata({\n            \"heading_path\": c[\"heading_path\"],\n            **c[\"metadata\"],\n        })\n        for c in chunks\n    ]\n\n    print(f\"[chroma] Adding {len(documents)} chunks ...\")\n\n    # 👉 If you ever hit provider/chroma limits on very large datasets,\n    #    you can add in smaller batches (e.g., batches of 1k):\n    # for i in range(0, len(documents), 1000):\n    #     db.add(\n    #         documents=documents[i:i+1000],\n    #         ids=ids[i:i+1000],\n    #         metadatas=metadatas[i:i+1000],\n    #     )\n\n    db.add(documents=documents, ids=ids, metadatas=metadatas)\n\n    print(f\"[chroma] Count: {db.count()}\")\n    return db, embed_fn\n\n\n# Build DB now\ndb, embed_fn = build_vector_db(chunks)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:02:17.085045Z","iopub.execute_input":"2025-09-05T13:02:17.085723Z","iopub.status.idle":"2025-09-05T13:02:20.579604Z","shell.execute_reply.started":"2025-09-05T13:02:17.085698Z","shell.execute_reply":"2025-09-05T13:02:20.578927Z"}},"outputs":[{"name":"stdout","text":"[chroma] Adding 308 chunks ...\n[chroma] Count: 308\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import json\nfrom typing import List, Dict, Any\n\ndef make_prompt(query: str, passages: List[str], metadatas: List[Dict[str, Any]]) -> str:\n    def _maybe_json(v):\n        if isinstance(v, str):\n            try:\n                return json.loads(v)\n            except Exception:\n                return v\n        return v\n\n    q_one = query.replace(\"\\n\", \" \")\n    prompt = (\n        \"You are a careful legal assistant. Answer ONLY from the reference passages below.\\n\"\n        \"Cite section numbers and chapter labels when available. If the answer cannot be found in the passages, say you don't know.\\n\"\n        \"Keep the answer concise and precise.\\n\\n\"\n        f\"QUESTION: {q_one}\\n\"\n    )\n\n    for i, (p, md) in enumerate(zip(passages, metadatas), start=1):\n        p_one = (p or \"\").replace(\"\\n\", \" \")\n\n        # Safely read metadata (some values may be JSON strings)\n        sec_no   = _maybe_json(md.get(\"section_number\", \"?\"))\n        sec_title= _maybe_json(md.get(\"section_title\", \"\"))\n        chapter  = _maybe_json(md.get(\"chapter\", \"\"))\n        pages    = _maybe_json(md.get(\"page_refs\", []))\n\n        prompt += (\n            f\"\\nPASSAGE {i} (Section {sec_no}: {sec_title} | {chapter} | \"\n            f\"pages {pages}): {p_one}\\n\"\n        )\n    return prompt\n\n\ndef rag_answer(db, embed_fn: GeminiEmbeddingFunction, query: str, top_k: int = 4, model: str = \"gemini-2.0-flash\"):\n    # Switch to query mode for embeddings\n    embed_fn.document_mode = False\n\n    # IMPORTANT: do NOT include \"ids\" here; chroma raises on that.\n    result = db.query(\n        query_texts=[query],\n        n_results=top_k,\n        include=[\"documents\", \"metadatas\", \"distances\"],  # no \"ids\" here\n    )\n\n    passages  = result.get(\"documents\", [[]])[0]\n    metadatas = result.get(\"metadatas\", [[]])[0]\n    distances = result.get(\"distances\", [[]])[0]\n    ids       = result.get(\"ids\", [[]])[0]  # safe: available even if not in include\n\n    prompt = make_prompt(query, passages, metadatas)\n    answer = client.models.generate_content(model=model, contents=prompt)\n\n    return {\n        \"answer\": getattr(answer, \"text\", str(answer)),\n        \"passages\": passages,\n        \"metadatas\": metadatas,\n        \"distances\": distances,\n        \"ids\": ids,\n        \"prompt\": prompt,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:02:25.993246Z","iopub.execute_input":"2025-09-05T13:02:25.994005Z","iopub.status.idle":"2025-09-05T13:02:26.002454Z","shell.execute_reply.started":"2025-09-05T13:02:25.993978Z","shell.execute_reply":"2025-09-05T13:02:26.001632Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import json\n\n# ------------------------------------------\n# 🧽 Helper: Attempt to parse JSON-encoded metadata fields\n# ------------------------------------------\ndef _maybe_json(v):\n    \"\"\"\n    Safely decode metadata fields that may be stored as JSON strings.\n    If parsing fails, return the original value.\n    \"\"\"\n    if isinstance(v, str):\n        try:\n            return json.loads(v)\n        except Exception:\n            return v\n    return v\n\n\n# ------------------------------------------\n# 🧪 DEMO QUERY\n# ------------------------------------------\ndemo_query = \"What does the Constitution say the role of the King?\"\n\n# Call our RAG pipeline\nresult = rag_answer(db, embed_fn, demo_query, top_k=4)\n\n# ------------------------------------------\n# 📝 Display Answer\n# ------------------------------------------\nprint(\"=== RAG ANSWER ===\\n\")\nprint(result.get(\"answer\", \"\").strip())\n\n# ------------------------------------------\n# 📚 Show Sources (for transparency & debugging)\n# ------------------------------------------\nprint(\"\\n--- Sources ---\")\nmetas = result.get(\"metadatas\", [])\ndists = result.get(\"distances\", [])\nids   = result.get(\"ids\", [])\n\nfor i, (md, d, cid) in enumerate(zip(metas, dists, ids), start=1):\n    # Attempt to decode any JSON fields back to Python objects\n    chapter   = _maybe_json(md.get(\"chapter\"))\n    sec_no    = _maybe_json(md.get(\"section_number\"))\n    sec_title = _maybe_json(md.get(\"section_title\"))\n    page_refs = _maybe_json(md.get(\"page_refs\"))\n\n    # Fallbacks: avoid printing \"None\"\n    chapter   = chapter or \"\"\n    sec_no    = sec_no or \"?\"\n    sec_title = sec_title or \"\"\n    page_refs = page_refs if isinstance(page_refs, list) else page_refs\n\n    # Print citation-like reference with similarity distance (lower = closer)\n    print(f\"[{i}] id={cid} | {chapter} | Section {sec_no}: {sec_title} | pages {page_refs} | distance={d:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T13:02:28.909101Z","iopub.execute_input":"2025-09-05T13:02:28.909430Z","iopub.status.idle":"2025-09-05T13:02:30.567099Z","shell.execute_reply.started":"2025-09-05T13:02:28.909405Z","shell.execute_reply":"2025-09-05T13:02:30.566258Z"}},"outputs":[{"name":"stdout","text":"=== RAG ANSWER ===\n\nThe executive authority of Swaziland vests in the King as Head of State and shall be exercised in accordance with the provisions of the Constitution (Section 64(1), CHAPTER VI). The King is a hereditary Head of State (Section 4(1), CHAPTER II) and a symbol of unity (Section 4(2), CHAPTER II). The King is also Commander-in-Chief of the Defence Force, Commissioner-in-Chief of the Police Service, and Commissioner-in-Chief of the Correctional Services (Section 4(3), CHAPTER II). The King and Parliament may make laws for the peace, order, and good government of Swaziland (Section 106(b), CHAPTER VII).\n\n--- Sources ---\n[1] id=6ad5e7a3-50ab-4703-bbf1-162fbf03bf24 | CHAPTER VI | Section 64: (1) The executive authority of Swaziland vests in the King as Head of State and | pages [50, 51] | distance=0.5717\n[2] id=1b98da3e-f0cc-4bac-b628-cf336ebbb5ff | CHAPTER VI | Section 65: (1) In the exercise of the functions under this Constitution or any other law the | pages [51] | distance=0.6567\n[3] id=fbb1484b-d3f5-4972-abe2-58687159c588 | CHAPTER II | Section 4: (1) Without prejudice to the provisions of section 228, King and iNgwenyama of | pages [14] | distance=0.6973\n[4] id=4ed3fc61-c4f4-4464-bbc3-3f2f03769947 | CHAPTER VII | Section 106: Subject to the provisions of this Constitution | pages [73] | distance=0.7035\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}